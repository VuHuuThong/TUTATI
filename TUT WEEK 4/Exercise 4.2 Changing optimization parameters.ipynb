{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1_mGa-W3e78x5ikzmIxKO4ydFOD7ONwK8","timestamp":1758092057825}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Exercise 4.2 Changing optimization parameters\n"],"metadata":{"id":"ApEGsGnec6BZ"}},{"cell_type":"markdown","source":["In this exercise, you will use the same titanic dataset from the previous exercises. You can download the dataset from the following link:\n","\n","[titanic_all_numeric.csv](https://drive.google.com/file/d/11nuYS-l3EXCsGJt81y4YTt3oTnFGaB68/view?usp=drive_link)\n","\n","The data is pre-loaded into a pandas DataFrame called `df`. The `predictors` and `target` values are also pre-defined.\n","\n","You'll want the optimization to start from scratch every time you change the learning rate, to give a fair comparison of how each learning rate did in your results. So we have created a function `get_new_model()` that creates an unoptimized model to optimize.\n","\n","It's time to get your hands dirty with optimization. You'll now try optimizing a model at a very low learning rate, a very high learning rate, and a \"just right\" learning rate. You'll want to look at the results after running this exercise, remembering that a low value for the loss function is good."],"metadata":{"id":"mEOOH6NdV42q"}},{"cell_type":"markdown","source":["## Instructions"],"metadata":{"id":"-AWPd5Wqft_d"}},{"cell_type":"markdown","source":["* Import `SGD` from `tensorflow.keras.optimizers`.\n","* Create a list of learning rates to try optimizing with called `lr_to_test`. The learning rates in it should be `0.000001`, `0.01`, and `1.0`.\n","* Using a `for` loop to iterate over `lr_to_test`:\n","  * Use the `get_new_model()` function to build a new, unoptimized model.\n","  * Create an optimizer called `my_optimizer` using the `SGD()` constructor with keyword argument `learning_rate=lr`.\n","  * Compile your model. Set the optimizer parameter to be the SGD object you created above, and because this is a classification problem, use `'categorical_crossentropy'` for the loss parameter, , and `metrics=['accuracy']` to see the accuracy at the end of each epoch.\n","  * Fit the model using the `predictors` and the `target`."],"metadata":{"id":"fNotC4abWBqN"}},{"cell_type":"markdown","source":["## Code"],"metadata":{"id":"oKx-pAyrlVGO"}},{"cell_type":"markdown","source":[],"metadata":{"id":"K9NYP4qnceGb"}},{"cell_type":"markdown","source":["Load data and convert the data to NumPy array:"],"metadata":{"id":"K2uQ5t2BRtpg"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"dlu9wWcWcy44","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758091891984,"user_tz":-420,"elapsed":5591,"user":{"displayName":"hoàng nguyễn","userId":"12062038586805712278"}},"outputId":"45f702e0-e4c2-4e71-f5fc-59fdf9dccdac"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-797114906.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df.age_was_missing = df.age_was_missing.replace({True: 1, False: 0})\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.utils import to_categorical\n","\n","# Load csv file into the dataframe: df\n","df = pd.read_csv(\"/content/titanic_all_numeric.csv\")\n","\n","# Convert the boolean values of the 'age_was_missing' column to integer\n","df.age_was_missing = df.age_was_missing.replace({True: 1, False: 0})\n","\n","# Create predictors NumPy array: predictors\n","predictors = df.drop(['survived'], axis=1).values\n","\n","# Save the number of columns in predictors: n_cols\n","n_cols = predictors.shape[1]\n","\n","# Convert the target to categorical: target\n","target = to_categorical(df['survived'])\n"]},{"cell_type":"markdown","source":["Create a neural network for a classification task"],"metadata":{"id":"9jf6A0pPsATU"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","\n","def get_new_model():\n","  # Set up the model\n","  model = Sequential()\n","  model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n","  model.add(Dense(2, activation='softmax'))\n","\n","  return model"],"metadata":{"id":"wkgdNpMmsKmt","executionInfo":{"status":"ok","timestamp":1758091897550,"user_tz":-420,"elapsed":12,"user":{"displayName":"hoàng nguyễn","userId":"12062038586805712278"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Optimize the model with different learning rate:"],"metadata":{"id":"u986qJ_NOK1h"}},{"cell_type":"code","source":["# Import the SGD optimizer\n","from tensorflow.keras.optimizers import SGD\n","\n","# Create list of learning rates: lr_to_test\n","lr_to_test = [0.000001, 0.01, 1.0]\n","\n","# Loop over learning rates\n","for lr in lr_to_test:\n","    print('\\n\\nTesting model with learning rate: %f\\n' % lr)\n","\n","    # Build new model to test, unaffected by previous models\n","    model = get_new_model()\n","\n","    # Create SGD optimizer with specified learning rate: my_optimizer\n","    my_optimizer = SGD(learning_rate=lr)\n","\n","    # Compile the model\n","    model.compile(optimizer=my_optimizer,\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    # Fit the model (1 epoch như trong đề)\n","    model.fit(predictors, target, epochs=1, batch_size=32)\n"],"metadata":{"id":"rdrPE2NrOjS5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758092051057,"user_tz":-420,"elapsed":1843,"user":{"displayName":"hoàng nguyễn","userId":"12062038586805712278"}},"outputId":"d3f6e2ef-3b9a-4e0e-eabc-3424423550dc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Testing model with learning rate: 0.000001\n","\n","\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5941 - loss: 4.3236\n","\n","\n","Testing model with learning rate: 0.010000\n","\n","\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5438 - loss: 4.5678\n","\n","\n","Testing model with learning rate: 1.000000\n","\n","\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5682 - loss: 562.2781 \n"]}]},{"cell_type":"markdown","source":["The ouput should be:\n","\n","Testing model with learning rate: 0.000001\n","\n","28/28 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.4074 - loss: 5.5751   \n","\n","\n","Testing model with learning rate: 0.010000\n","\n","28/28 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.5747 - loss: 3.3378   \n","\n","\n","Testing model with learning rate: 1.000000\n","\n","28/28 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.5404 - loss: 28697.6289\n"],"metadata":{"id":"ecubSk7GZKKp"}}]}